{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GMMX: Gaussian Mixture Models in Jax","text":"<p>A minimal implementation of Gaussian Mixture Models in Jax.</p>"},{"location":"development/","title":"Development","text":""},{"location":"development/#getting-started-with-your-project","title":"Getting started with your project","text":""},{"location":"development/#1-create-a-new-repository","title":"1. Create a New Repository","text":"<p>First, create a repository on GitHub with the same name as this project, and then run the following commands:</p> <pre><code>git init -b main\ngit add .\ngit commit -m \"init commit\"\ngit remote add origin git@github.com:adonath/gmmx.git\ngit push -u origin main\n</code></pre>"},{"location":"development/#2-set-up-your-development-environment","title":"2. Set Up Your Development Environment","text":"<p>Then, install the environment and the pre-commit hooks with</p> <pre><code>make install\n</code></pre> <p>This will also generate your <code>uv.lock</code> file</p>"},{"location":"development/#3-run-the-pre-commit-hooks","title":"3. Run the pre-commit hooks","text":"<p>Initially, the CI/CD pipeline might be failing due to formatting issues. To resolve those run:</p> <pre><code>uv run pre-commit run -a\n</code></pre>"},{"location":"development/#4-commit-the-changes","title":"4. Commit the changes","text":"<p>Lastly, commit the changes made by the two steps above to your repository.</p> <pre><code>git add .\ngit commit -m 'Fix formatting issues'\ngit push origin main\n</code></pre> <p>You are now ready to start development on your project! The CI/CD pipeline will be triggered when you open a pull request, merge to main, or when you create a new release.</p> <p>To finalize the set-up for publishing to PyPI, see here. For activating the automatic documentation with MkDocs, see here. To enable the code coverage reports, see here.</p>"},{"location":"development/#releasing-a-new-version","title":"Releasing a new version","text":"<ul> <li>Create an API Token on PyPI.</li> <li>Add the API Token to your projects secrets with the name <code>PYPI_TOKEN</code> by visiting this page.</li> <li>Create a new release on Github.</li> <li>Create a new tag in the form <code>*.*.*</code>.</li> </ul> <p>For more details, see here.</p> <p>Repository initiated with fpgmaas/cookiecutter-uv.</p>"},{"location":"modules/","title":"Modules","text":"<p>Some notes on the implementation:</p> <p>I have not tried to keep the implementation close to the sklearn implementation. I have rather tried to realize my own best practices for code structure and clarity. Here are some more detailed thoughts:</p> <ol> <li> <p>Use dataclasses for the model representation: this reduces the amount of boilerplate code for initialization and in combination with the <code>register_dataclass_jax</code> decorator it integrates seamleassly with JAX.</p> </li> <li> <p>Split up the different covariance types into different classes: this avoids the need for multiple blocks of if-else statements.</p> </li> <li> <p>Use a registry for the covariance types:  This allows for easy extensibility by the user.</p> </li> <li> <p>Remove Python loops: I have not checked the reason why the sklearn implementation still uses Python loops, but my guess is that it is simpler(?) and when there are operations such as matmul and cholesky decomposition, the Python loop does not become the bottleneck. In JAX, however, it is usually better to avoid Python loops and let the JAX compiler take care of the optimization instead.</p> </li> <li> <p>Rely on same internal array dimension and axis order: Internally all(!) involved arrays (even 1d weights) are represented as 4d arrays with the axes (batch, components, features, features_covar). This makes it much easier to write array operations and rely on broadcasting. This minimizes the amount of in-line reshaping and in-line extension of dimensions. If you think about it, this is most likely the way how array programming was meant to be used in first place. Yet, I have rarely seen this in practice, probably because people struggle with the additional dimensions in the beginning. However once you get used to it, it is much easier to write and understand the code! The only downside is that the user has to face the additional \"empty\" dimensions when directy working with the arrays. For convenience I have introduced properties, that return the arrays with the empty dimensions removed. Another downside maybe that you have to use <code>keepdims=True</code> more often, but there I would even argue that the default behavior in the array libraries should change.</p> </li> <li> <p>\"Poor-peoples\" named axes: The axis order convention is defined in the code in the <code>Axis</code> enum, which maps the name to the integer dimension. Later I can use, e.g. <code>Axis.batch</code> to refer to the batch axis in the code. This is the simplest way to come close to named axes in any array library! So you can use e.g. <code>jnp.sum(x, axes=Axis.components)</code> to sum over the components axis. I found this to be a very powerful concept that improves the code clarity a lot, yet I have not seen it often in other libraries. Of course there is <code>einops</code> but the simple enum works just fine in many cases!</p> </li> </ol>"},{"location":"modules/#gmmx.gmm.Axis","title":"<code>Axis</code>","text":"<p>               Bases: <code>int</code>, <code>Enum</code></p> <p>Internal axis order</p> Source code in <code>gmmx/gmm.py</code> <pre><code>class Axis(int, Enum):\n    \"\"\"Internal axis order\"\"\"\n\n    batch = 0\n    components = 1\n    features = 2\n    features_covar = 3\n</code></pre>"},{"location":"modules/#gmmx.gmm.CovarianceType","title":"<code>CovarianceType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Convariance type</p> Source code in <code>gmmx/gmm.py</code> <pre><code>class CovarianceType(str, Enum):\n    \"\"\"Convariance type\"\"\"\n\n    full = \"full\"\n    tied = \"tied\"\n    diag = \"diag\"\n    spherical = \"spherical\"\n</code></pre>"},{"location":"modules/#gmmx.gmm.FullCovariances","title":"<code>FullCovariances</code>  <code>dataclass</code>","text":"<p>Full covariance matrix</p>"},{"location":"modules/#gmmx.gmm.FullCovariances--attributes","title":"Attributes","text":"<p>values : jax.array     Covariance values. Expected shape is (1, n_components, n_features, n_features)</p> Source code in <code>gmmx/gmm.py</code> <pre><code>@register_dataclass_jax(data_fields=[\"values\"])\n@dataclass\nclass FullCovariances:\n    \"\"\"Full covariance matrix\n\n    Attributes\n    ----------\n    values : jax.array\n        Covariance values. Expected shape is (1, n_components, n_features, n_features)\n    \"\"\"\n\n    values: jax.Array\n\n    def __post_init__(self) -&gt; None:\n        check_shape(self.values, (1, None, None, None))\n\n    @classmethod\n    def from_squeezed(cls, values: AnyArray) -&gt; FullCovariances:\n        \"\"\"Create a covariance matrix from squeezed array\n\n        Parameters\n        ----------\n        values : jax.Array ot np.array\n            Covariance values. Expected shape is (n_components, n_features, n_features)\n\n        Returns\n        -------\n        covariances : FullCovariances\n            Covariance matrix instance.\n        \"\"\"\n        return cls(values=jnp.expand_dims(values, axis=Axis.batch))\n\n    @property\n    def values_numpy(self) -&gt; np.ndarray:\n        \"\"\"Covariance as numpy array\"\"\"\n        return np.squeeze(np.asarray(self.values), axis=Axis.batch)\n\n    @property\n    def precisions_cholesky_numpy(self) -&gt; np.ndarray:\n        \"\"\"Compute precision matrices\"\"\"\n        return np.squeeze(np.asarray(self.precisions_cholesky), axis=Axis.batch)\n\n    @classmethod\n    def create(\n        cls, n_components: int, n_features: int, device: Device = None\n    ) -&gt; FullCovariances:\n        \"\"\"Create covariance matrix\n\n        By default the covariance matrix is set to the identity matrix.\n\n        Parameters\n        ----------\n        n_components : int\n            Number of components\n        n_features : int\n            Number of features\n        device : str, optional\n            Device, by default None\n\n        Returns\n        -------\n        covariances : FullCovariances\n            Covariance matrix instance.\n        \"\"\"\n        identity = jnp.expand_dims(\n            jnp.eye(n_features), axis=(Axis.batch, Axis.components)\n        )\n\n        values = jnp.repeat(identity, n_components, axis=Axis.components)\n        values = jax.device_put(values, device=device)\n        return cls(values=values)\n\n    def log_prob(self, x: jax.Array, means: jax.Array) -&gt; jax.Array:\n        \"\"\"Compute log likelihood from the covariance for a given feature vector\n\n        Parameters\n        ----------\n        x : jax.array\n            Feature vectors\n        means : jax.array\n            Means of the components\n\n        Returns\n        -------\n        log_prob : jax.array\n            Log likelihood\n        \"\"\"\n        precisions_cholesky = self.precisions_cholesky\n\n        y = jnp.matmul(x.mT, precisions_cholesky) - jnp.matmul(\n            means.mT, precisions_cholesky\n        )\n        return jnp.sum(\n            jnp.square(y),\n            axis=(Axis.features, Axis.features_covar),\n            keepdims=True,\n        )\n\n    def update_parameters(\n        self,\n        x: jax.Array,\n        means: jax.Array,\n        resp: jax.Array,\n        nk: jax.Array,\n        reg_covar: float,\n    ) -&gt; FullCovariances:\n        \"\"\"Estimate updated covariance matrix from data\n\n        Parameters\n        ----------\n        x : jax.array\n            Feature vectors\n        means : jax.array\n            Means of the components\n        resp : jax.array\n            Responsibilities\n        nk : jax.array\n            Number of samples in each component\n        reg_covar : float\n            Regularization for the covariance matrix\n\n        Returns\n        -------\n        covariances : FullCovariances\n            Updated covariance matrix instance.\n        \"\"\"\n        diff = x - means\n        axes = (Axis.features_covar, Axis.components, Axis.features, Axis.batch)\n        diff = jnp.transpose(diff, axes=axes)\n        resp = jnp.transpose(resp, axes=axes)\n        values = jnp.matmul(resp * diff, diff.mT) / nk\n        idx = jnp.arange(self.n_features)\n        values = values.at[:, :, idx, idx].add(reg_covar)\n        return self.__class__(values=values)\n\n    @property\n    def n_components(self) -&gt; int:\n        \"\"\"Number of components\"\"\"\n        return self.values.shape[Axis.components]\n\n    @property\n    def n_features(self) -&gt; int:\n        \"\"\"Number of features\"\"\"\n        return self.values.shape[Axis.features]\n\n    @property\n    def n_parameters(self) -&gt; int:\n        \"\"\"Number of parameters\"\"\"\n        return int(self.n_components * self.n_features * (self.n_features + 1) / 2.0)\n\n    @property\n    def log_det_cholesky(self) -&gt; jax.Array:\n        \"\"\"Log determinant of the cholesky decomposition\"\"\"\n        diag = jnp.trace(\n            jnp.log(self.precisions_cholesky),\n            axis1=Axis.features,\n            axis2=Axis.features_covar,\n        )\n        return jnp.expand_dims(diag, axis=(Axis.features, Axis.features_covar))\n\n    @property\n    def precisions_cholesky(self) -&gt; jax.Array:\n        \"\"\"Compute precision matrices\"\"\"\n        cov_chol = jsp.linalg.cholesky(self.values, lower=True)\n\n        identity = jnp.expand_dims(\n            jnp.eye(self.n_features), axis=(Axis.batch, Axis.components)\n        )\n\n        b = jnp.repeat(identity, self.n_components, axis=Axis.components)\n        precisions_chol = jsp.linalg.solve_triangular(cov_chol, b, lower=True)\n        return precisions_chol.mT\n</code></pre>"},{"location":"modules/#gmmx.gmm.FullCovariances.log_det_cholesky","title":"<code>log_det_cholesky: jax.Array</code>  <code>property</code>","text":"<p>Log determinant of the cholesky decomposition</p>"},{"location":"modules/#gmmx.gmm.FullCovariances.n_components","title":"<code>n_components: int</code>  <code>property</code>","text":"<p>Number of components</p>"},{"location":"modules/#gmmx.gmm.FullCovariances.n_features","title":"<code>n_features: int</code>  <code>property</code>","text":"<p>Number of features</p>"},{"location":"modules/#gmmx.gmm.FullCovariances.n_parameters","title":"<code>n_parameters: int</code>  <code>property</code>","text":"<p>Number of parameters</p>"},{"location":"modules/#gmmx.gmm.FullCovariances.precisions_cholesky","title":"<code>precisions_cholesky: jax.Array</code>  <code>property</code>","text":"<p>Compute precision matrices</p>"},{"location":"modules/#gmmx.gmm.FullCovariances.precisions_cholesky_numpy","title":"<code>precisions_cholesky_numpy: np.ndarray</code>  <code>property</code>","text":"<p>Compute precision matrices</p>"},{"location":"modules/#gmmx.gmm.FullCovariances.values_numpy","title":"<code>values_numpy: np.ndarray</code>  <code>property</code>","text":"<p>Covariance as numpy array</p>"},{"location":"modules/#gmmx.gmm.FullCovariances.create","title":"<code>create(n_components, n_features, device=None)</code>  <code>classmethod</code>","text":"<p>Create covariance matrix</p> <p>By default the covariance matrix is set to the identity matrix.</p>"},{"location":"modules/#gmmx.gmm.FullCovariances.create--parameters","title":"Parameters","text":"<p>n_components : int     Number of components n_features : int     Number of features device : str, optional     Device, by default None</p>"},{"location":"modules/#gmmx.gmm.FullCovariances.create--returns","title":"Returns","text":"<p>covariances : FullCovariances     Covariance matrix instance.</p> Source code in <code>gmmx/gmm.py</code> <pre><code>@classmethod\ndef create(\n    cls, n_components: int, n_features: int, device: Device = None\n) -&gt; FullCovariances:\n    \"\"\"Create covariance matrix\n\n    By default the covariance matrix is set to the identity matrix.\n\n    Parameters\n    ----------\n    n_components : int\n        Number of components\n    n_features : int\n        Number of features\n    device : str, optional\n        Device, by default None\n\n    Returns\n    -------\n    covariances : FullCovariances\n        Covariance matrix instance.\n    \"\"\"\n    identity = jnp.expand_dims(\n        jnp.eye(n_features), axis=(Axis.batch, Axis.components)\n    )\n\n    values = jnp.repeat(identity, n_components, axis=Axis.components)\n    values = jax.device_put(values, device=device)\n    return cls(values=values)\n</code></pre>"},{"location":"modules/#gmmx.gmm.FullCovariances.from_squeezed","title":"<code>from_squeezed(values)</code>  <code>classmethod</code>","text":"<p>Create a covariance matrix from squeezed array</p>"},{"location":"modules/#gmmx.gmm.FullCovariances.from_squeezed--parameters","title":"Parameters","text":"<p>values : jax.Array ot np.array     Covariance values. Expected shape is (n_components, n_features, n_features)</p>"},{"location":"modules/#gmmx.gmm.FullCovariances.from_squeezed--returns","title":"Returns","text":"<p>covariances : FullCovariances     Covariance matrix instance.</p> Source code in <code>gmmx/gmm.py</code> <pre><code>@classmethod\ndef from_squeezed(cls, values: AnyArray) -&gt; FullCovariances:\n    \"\"\"Create a covariance matrix from squeezed array\n\n    Parameters\n    ----------\n    values : jax.Array ot np.array\n        Covariance values. Expected shape is (n_components, n_features, n_features)\n\n    Returns\n    -------\n    covariances : FullCovariances\n        Covariance matrix instance.\n    \"\"\"\n    return cls(values=jnp.expand_dims(values, axis=Axis.batch))\n</code></pre>"},{"location":"modules/#gmmx.gmm.FullCovariances.log_prob","title":"<code>log_prob(x, means)</code>","text":"<p>Compute log likelihood from the covariance for a given feature vector</p>"},{"location":"modules/#gmmx.gmm.FullCovariances.log_prob--parameters","title":"Parameters","text":"<p>x : jax.array     Feature vectors means : jax.array     Means of the components</p>"},{"location":"modules/#gmmx.gmm.FullCovariances.log_prob--returns","title":"Returns","text":"<p>log_prob : jax.array     Log likelihood</p> Source code in <code>gmmx/gmm.py</code> <pre><code>def log_prob(self, x: jax.Array, means: jax.Array) -&gt; jax.Array:\n    \"\"\"Compute log likelihood from the covariance for a given feature vector\n\n    Parameters\n    ----------\n    x : jax.array\n        Feature vectors\n    means : jax.array\n        Means of the components\n\n    Returns\n    -------\n    log_prob : jax.array\n        Log likelihood\n    \"\"\"\n    precisions_cholesky = self.precisions_cholesky\n\n    y = jnp.matmul(x.mT, precisions_cholesky) - jnp.matmul(\n        means.mT, precisions_cholesky\n    )\n    return jnp.sum(\n        jnp.square(y),\n        axis=(Axis.features, Axis.features_covar),\n        keepdims=True,\n    )\n</code></pre>"},{"location":"modules/#gmmx.gmm.FullCovariances.update_parameters","title":"<code>update_parameters(x, means, resp, nk, reg_covar)</code>","text":"<p>Estimate updated covariance matrix from data</p>"},{"location":"modules/#gmmx.gmm.FullCovariances.update_parameters--parameters","title":"Parameters","text":"<p>x : jax.array     Feature vectors means : jax.array     Means of the components resp : jax.array     Responsibilities nk : jax.array     Number of samples in each component reg_covar : float     Regularization for the covariance matrix</p>"},{"location":"modules/#gmmx.gmm.FullCovariances.update_parameters--returns","title":"Returns","text":"<p>covariances : FullCovariances     Updated covariance matrix instance.</p> Source code in <code>gmmx/gmm.py</code> <pre><code>def update_parameters(\n    self,\n    x: jax.Array,\n    means: jax.Array,\n    resp: jax.Array,\n    nk: jax.Array,\n    reg_covar: float,\n) -&gt; FullCovariances:\n    \"\"\"Estimate updated covariance matrix from data\n\n    Parameters\n    ----------\n    x : jax.array\n        Feature vectors\n    means : jax.array\n        Means of the components\n    resp : jax.array\n        Responsibilities\n    nk : jax.array\n        Number of samples in each component\n    reg_covar : float\n        Regularization for the covariance matrix\n\n    Returns\n    -------\n    covariances : FullCovariances\n        Updated covariance matrix instance.\n    \"\"\"\n    diff = x - means\n    axes = (Axis.features_covar, Axis.components, Axis.features, Axis.batch)\n    diff = jnp.transpose(diff, axes=axes)\n    resp = jnp.transpose(resp, axes=axes)\n    values = jnp.matmul(resp * diff, diff.mT) / nk\n    idx = jnp.arange(self.n_features)\n    values = values.at[:, :, idx, idx].add(reg_covar)\n    return self.__class__(values=values)\n</code></pre>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax","title":"<code>GaussianMixtureModelJax</code>  <code>dataclass</code>","text":"<p>Gaussian Mixture Model</p>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax--attributes","title":"Attributes","text":"<p>weights : jax.array     Weights of each component. Expected shape is (1, n_components, 1, 1) means : jax.array     Mean of each component. Expected shape is (1, n_components, n_features, 1) covariances : jax.array     Covariance of each component. Expected shape is (1, n_components, n_features, n_features)</p> Source code in <code>gmmx/gmm.py</code> <pre><code>@register_dataclass_jax(data_fields=[\"weights\", \"means\", \"covariances\"])\n@dataclass\nclass GaussianMixtureModelJax:\n    \"\"\"Gaussian Mixture Model\n\n    Attributes\n    ----------\n    weights : jax.array\n        Weights of each component. Expected shape is (1, n_components, 1, 1)\n    means : jax.array\n        Mean of each component. Expected shape is (1, n_components, n_features, 1)\n    covariances : jax.array\n        Covariance of each component. Expected shape is (1, n_components, n_features, n_features)\n    \"\"\"\n\n    weights: jax.Array\n    means: jax.Array\n    covariances: FullCovariances\n\n    def __post_init__(self) -&gt; None:\n        check_shape(self.weights, (1, None, 1, 1))\n        check_shape(self.means, (1, None, None, 1))\n\n    @property\n    def weights_numpy(self) -&gt; np.ndarray:\n        \"\"\"Weights as numpy array\"\"\"\n        return np.squeeze(\n            np.asarray(self.weights),\n            axis=(Axis.batch, Axis.features, Axis.features_covar),\n        )\n\n    @property\n    def means_numpy(self) -&gt; np.ndarray:\n        \"\"\"Means as numpy array\"\"\"\n        return np.squeeze(\n            np.asarray(self.means), axis=(Axis.batch, Axis.features_covar)\n        )\n\n    @classmethod\n    def create(\n        cls,\n        n_components: int,\n        n_features: int,\n        covariance_type: CovarianceType = CovarianceType.full,\n        device: Device = None,\n    ) -&gt; GaussianMixtureModelJax:\n        \"\"\"Create a GMM from configuration\n\n        Parameters\n        ----------\n        n_components : int\n            Number of components\n        n_features : int\n            Number of features\n        covariance_type : str, optional\n            Covariance type, by default \"full\"\n        device : str, optional\n            Device, by default None\n\n        Returns\n        -------\n        gmm : GaussianMixtureModelJax\n            Gaussian mixture model instance.\n        \"\"\"\n        covariance_type = CovarianceType(covariance_type)\n\n        weights = jnp.ones((1, n_components, 1, 1)) / n_components\n        means = jnp.zeros((1, n_components, n_features, 1))\n        covariances = COVARIANCE[covariance_type].create(n_components, n_features)\n        return cls(\n            weights=jax.device_put(weights, device=device),\n            means=jax.device_put(means, device=device),\n            covariances=jax.device_put(covariances, device=device),\n        )\n\n    @classmethod\n    def from_squeezed(\n        cls,\n        means: AnyArray,\n        covariances: AnyArray,\n        weights: AnyArray,\n        covariance_type: CovarianceType = CovarianceType.full,\n    ) -&gt; GaussianMixtureModelJax:\n        \"\"\"Create a Jax GMM from squeezed arrays\n\n        Parameters\n        ----------\n        means : jax.Array or np.array\n            Mean of each component. Expected shape is (n_components, n_features)\n        covariances : jax.Array or np.array\n            Covariance of each component. Expected shape is (n_components, n_features, n_features)\n        weights : jax.Array or np.array\n            Weights of each component. Expected shape is (n_components,)\n        covariance_type : str, optional\n            Covariance type, by default \"full\"\n\n        Returns\n        -------\n        gmm : GaussianMixtureModelJax\n            Gaussian mixture model instance.\n        \"\"\"\n        covariance_type = CovarianceType(covariance_type)\n\n        means = jnp.expand_dims(means, axis=(Axis.batch, Axis.features_covar))\n        weights = jnp.expand_dims(\n            weights, axis=(Axis.batch, Axis.features, Axis.features_covar)\n        )\n\n        values = jnp.expand_dims(covariances, axis=Axis.batch)\n        covariances = COVARIANCE[covariance_type](values=values)\n        return cls(weights=weights, means=means, covariances=covariances)  # type: ignore [arg-type]\n\n    def update_parameters(\n        self, x: jax.Array, resp: jax.Array, reg_covar: float\n    ) -&gt; GaussianMixtureModelJax:\n        \"\"\"Update parameters\n\n        Parameters\n        ----------\n        x : jax.array\n            Feature vectors\n        resp : jax.array\n            Responsibilities\n        reg_covar : float\n            Regularization for the covariance matrix\n\n        Returns\n        -------\n        gmm : GaussianMixtureModelJax\n            Updated Gaussian mixture model\n        \"\"\"\n        nk = jnp.sum(resp, axis=Axis.batch, keepdims=True)\n        means = jnp.matmul(resp.T, x.T.mT).T / nk\n        covariances = self.covariances.update_parameters(\n            x=x, means=means, resp=resp, nk=nk, reg_covar=reg_covar\n        )\n        return self.__class__(\n            weights=nk / nk.sum(), means=means, covariances=covariances\n        )\n\n    @classmethod\n    def from_k_means(cls, x: jax.Array, n_components: int) -&gt; None:\n        \"\"\"Init from k-means clustering\n\n        Parameters\n        ----------\n        x : jax.array\n            Feature vectors\n        n_components : int\n            Number of components\n\n        Returns\n        -------\n        gmm : GaussianMixtureModelJax\n            Gaussian mixture model instance.\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def n_features(self) -&gt; int:\n        \"\"\"Number of features\"\"\"\n        return self.covariances.n_features\n\n    @property\n    def n_components(self) -&gt; int:\n        \"\"\"Number of components\"\"\"\n        return self.covariances.n_components\n\n    @property\n    def n_parameters(self) -&gt; int:\n        \"\"\"Number of parameters\"\"\"\n        return int(\n            self.n_components\n            + self.n_components * self.n_features\n            + self.covariances.n_parameters\n            - 1\n        )\n\n    @property\n    def log_weights(self) -&gt; jax.Array:\n        \"\"\"Log weights (~jax.ndarray)\"\"\"\n        return jnp.log(self.weights)\n\n    @jax.jit\n    def estimate_log_prob(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Compute log likelihood for given feature vector\n\n        Parameters\n        ----------\n        x : jax.array\n            Feature vectors\n\n        Returns\n        -------\n        log_prob : jax.array\n            Log likelihood\n        \"\"\"\n        x = jnp.expand_dims(x, axis=(Axis.components, Axis.features_covar))\n        log_prob = self.covariances.log_prob(x, self.means)\n        two_pi = jnp.array(2 * jnp.pi)\n\n        value = (\n            -0.5 * (self.n_features * jnp.log(two_pi) + log_prob)\n            + self.covariances.log_det_cholesky\n            + self.log_weights\n        )\n        return value\n\n    def to_sklearn(self, **kwargs: dict) -&gt; Any:\n        \"\"\"Convert to sklearn GaussianMixture\n\n        The methods sets the weights, means, precisions_cholesky and covariances_ attributes,\n        however sklearn will overvwrite them when fitting the model.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Additional arguments passed to `~sklearn.mixture.GaussianMixture`\n\n        Returns\n        -------\n        gmm : `~sklearn.mixture.GaussianMixture`\n            Gaussian mixture model instance.\n        \"\"\"\n        from sklearn.mixture import GaussianMixture  # type: ignore [import-untyped]\n\n        gmm = GaussianMixture(\n            n_components=self.n_components,\n            covariance_type=SKLEARN_COVARIANCE_TYPE[type(self.covariances)],\n            **kwargs,\n        )\n        gmm.weights_ = self.weights_numpy\n        gmm.means_ = self.means_numpy\n        gmm.precisions_cholesky_ = self.covariances.precisions_cholesky_numpy\n        gmm.covariances_ = self.covariances.values_numpy\n        return gmm\n\n    @jax.jit\n    def predict(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Predict the component index for each sample\n\n        Parameters\n        ----------\n        x : jax.array\n            Feature vectors\n\n        Returns\n        -------\n        predictions : jax.array\n            Predicted component index\n        \"\"\"\n        log_prob = self.estimate_log_prob(x)\n        predictions = jnp.argmax(log_prob, axis=Axis.components, keepdims=True)\n        return jnp.squeeze(predictions, axis=(Axis.features, Axis.features_covar))\n\n    @partial(jax.jit, static_argnames=[\"n_samples\"])\n    def sample(self, key: jax.Array, n_samples: int) -&gt; jax.Array:\n        \"\"\"Sample from the model\n\n        Parameters\n        ----------\n        key : jax.random.PRNGKey\n            Random key\n        n_samples : int\n            Number of samples\n\n        Returns\n        -------\n        samples : jax.array\n            Samples\n        \"\"\"\n        key, subkey = jax.random.split(key)\n\n        selected = jax.random.choice(\n            key,\n            jnp.arange(self.n_components),\n            p=self.weights.flatten(),\n            shape=(n_samples,),\n        )\n\n        # TODO: this blows up the memory, as the arrays are copied\n        means = jnp.take(self.means, selected, axis=Axis.components)\n        covar = jnp.take(self.covariances.values, selected, axis=Axis.components)\n\n        samples = jax.random.multivariate_normal(\n            subkey,\n            jnp.squeeze(means, axis=(Axis.batch, Axis.features_covar)),\n            jnp.squeeze(covar, axis=Axis.batch),\n            shape=(n_samples,),\n        )\n\n        return samples\n</code></pre>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.log_weights","title":"<code>log_weights: jax.Array</code>  <code>property</code>","text":"<p>Log weights (~jax.ndarray)</p>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.means_numpy","title":"<code>means_numpy: np.ndarray</code>  <code>property</code>","text":"<p>Means as numpy array</p>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.n_components","title":"<code>n_components: int</code>  <code>property</code>","text":"<p>Number of components</p>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.n_features","title":"<code>n_features: int</code>  <code>property</code>","text":"<p>Number of features</p>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.n_parameters","title":"<code>n_parameters: int</code>  <code>property</code>","text":"<p>Number of parameters</p>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.weights_numpy","title":"<code>weights_numpy: np.ndarray</code>  <code>property</code>","text":"<p>Weights as numpy array</p>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.create","title":"<code>create(n_components, n_features, covariance_type=CovarianceType.full, device=None)</code>  <code>classmethod</code>","text":"<p>Create a GMM from configuration</p>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.create--parameters","title":"Parameters","text":"<p>n_components : int     Number of components n_features : int     Number of features covariance_type : str, optional     Covariance type, by default \"full\" device : str, optional     Device, by default None</p>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.create--returns","title":"Returns","text":"<p>gmm : GaussianMixtureModelJax     Gaussian mixture model instance.</p> Source code in <code>gmmx/gmm.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    n_components: int,\n    n_features: int,\n    covariance_type: CovarianceType = CovarianceType.full,\n    device: Device = None,\n) -&gt; GaussianMixtureModelJax:\n    \"\"\"Create a GMM from configuration\n\n    Parameters\n    ----------\n    n_components : int\n        Number of components\n    n_features : int\n        Number of features\n    covariance_type : str, optional\n        Covariance type, by default \"full\"\n    device : str, optional\n        Device, by default None\n\n    Returns\n    -------\n    gmm : GaussianMixtureModelJax\n        Gaussian mixture model instance.\n    \"\"\"\n    covariance_type = CovarianceType(covariance_type)\n\n    weights = jnp.ones((1, n_components, 1, 1)) / n_components\n    means = jnp.zeros((1, n_components, n_features, 1))\n    covariances = COVARIANCE[covariance_type].create(n_components, n_features)\n    return cls(\n        weights=jax.device_put(weights, device=device),\n        means=jax.device_put(means, device=device),\n        covariances=jax.device_put(covariances, device=device),\n    )\n</code></pre>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.estimate_log_prob","title":"<code>estimate_log_prob(x)</code>","text":"<p>Compute log likelihood for given feature vector</p>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.estimate_log_prob--parameters","title":"Parameters","text":"<p>x : jax.array     Feature vectors</p>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.estimate_log_prob--returns","title":"Returns","text":"<p>log_prob : jax.array     Log likelihood</p> Source code in <code>gmmx/gmm.py</code> <pre><code>@jax.jit\ndef estimate_log_prob(self, x: jax.Array) -&gt; jax.Array:\n    \"\"\"Compute log likelihood for given feature vector\n\n    Parameters\n    ----------\n    x : jax.array\n        Feature vectors\n\n    Returns\n    -------\n    log_prob : jax.array\n        Log likelihood\n    \"\"\"\n    x = jnp.expand_dims(x, axis=(Axis.components, Axis.features_covar))\n    log_prob = self.covariances.log_prob(x, self.means)\n    two_pi = jnp.array(2 * jnp.pi)\n\n    value = (\n        -0.5 * (self.n_features * jnp.log(two_pi) + log_prob)\n        + self.covariances.log_det_cholesky\n        + self.log_weights\n    )\n    return value\n</code></pre>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.from_k_means","title":"<code>from_k_means(x, n_components)</code>  <code>classmethod</code>","text":"<p>Init from k-means clustering</p>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.from_k_means--parameters","title":"Parameters","text":"<p>x : jax.array     Feature vectors n_components : int     Number of components</p>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.from_k_means--returns","title":"Returns","text":"<p>gmm : GaussianMixtureModelJax     Gaussian mixture model instance.</p> Source code in <code>gmmx/gmm.py</code> <pre><code>@classmethod\ndef from_k_means(cls, x: jax.Array, n_components: int) -&gt; None:\n    \"\"\"Init from k-means clustering\n\n    Parameters\n    ----------\n    x : jax.array\n        Feature vectors\n    n_components : int\n        Number of components\n\n    Returns\n    -------\n    gmm : GaussianMixtureModelJax\n        Gaussian mixture model instance.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.from_squeezed","title":"<code>from_squeezed(means, covariances, weights, covariance_type=CovarianceType.full)</code>  <code>classmethod</code>","text":"<p>Create a Jax GMM from squeezed arrays</p>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.from_squeezed--parameters","title":"Parameters","text":"<p>means : jax.Array or np.array     Mean of each component. Expected shape is (n_components, n_features) covariances : jax.Array or np.array     Covariance of each component. Expected shape is (n_components, n_features, n_features) weights : jax.Array or np.array     Weights of each component. Expected shape is (n_components,) covariance_type : str, optional     Covariance type, by default \"full\"</p>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.from_squeezed--returns","title":"Returns","text":"<p>gmm : GaussianMixtureModelJax     Gaussian mixture model instance.</p> Source code in <code>gmmx/gmm.py</code> <pre><code>@classmethod\ndef from_squeezed(\n    cls,\n    means: AnyArray,\n    covariances: AnyArray,\n    weights: AnyArray,\n    covariance_type: CovarianceType = CovarianceType.full,\n) -&gt; GaussianMixtureModelJax:\n    \"\"\"Create a Jax GMM from squeezed arrays\n\n    Parameters\n    ----------\n    means : jax.Array or np.array\n        Mean of each component. Expected shape is (n_components, n_features)\n    covariances : jax.Array or np.array\n        Covariance of each component. Expected shape is (n_components, n_features, n_features)\n    weights : jax.Array or np.array\n        Weights of each component. Expected shape is (n_components,)\n    covariance_type : str, optional\n        Covariance type, by default \"full\"\n\n    Returns\n    -------\n    gmm : GaussianMixtureModelJax\n        Gaussian mixture model instance.\n    \"\"\"\n    covariance_type = CovarianceType(covariance_type)\n\n    means = jnp.expand_dims(means, axis=(Axis.batch, Axis.features_covar))\n    weights = jnp.expand_dims(\n        weights, axis=(Axis.batch, Axis.features, Axis.features_covar)\n    )\n\n    values = jnp.expand_dims(covariances, axis=Axis.batch)\n    covariances = COVARIANCE[covariance_type](values=values)\n    return cls(weights=weights, means=means, covariances=covariances)  # type: ignore [arg-type]\n</code></pre>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.predict","title":"<code>predict(x)</code>","text":"<p>Predict the component index for each sample</p>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.predict--parameters","title":"Parameters","text":"<p>x : jax.array     Feature vectors</p>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.predict--returns","title":"Returns","text":"<p>predictions : jax.array     Predicted component index</p> Source code in <code>gmmx/gmm.py</code> <pre><code>@jax.jit\ndef predict(self, x: jax.Array) -&gt; jax.Array:\n    \"\"\"Predict the component index for each sample\n\n    Parameters\n    ----------\n    x : jax.array\n        Feature vectors\n\n    Returns\n    -------\n    predictions : jax.array\n        Predicted component index\n    \"\"\"\n    log_prob = self.estimate_log_prob(x)\n    predictions = jnp.argmax(log_prob, axis=Axis.components, keepdims=True)\n    return jnp.squeeze(predictions, axis=(Axis.features, Axis.features_covar))\n</code></pre>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.sample","title":"<code>sample(key, n_samples)</code>","text":"<p>Sample from the model</p>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.sample--parameters","title":"Parameters","text":"<p>key : jax.random.PRNGKey     Random key n_samples : int     Number of samples</p>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.sample--returns","title":"Returns","text":"<p>samples : jax.array     Samples</p> Source code in <code>gmmx/gmm.py</code> <pre><code>@partial(jax.jit, static_argnames=[\"n_samples\"])\ndef sample(self, key: jax.Array, n_samples: int) -&gt; jax.Array:\n    \"\"\"Sample from the model\n\n    Parameters\n    ----------\n    key : jax.random.PRNGKey\n        Random key\n    n_samples : int\n        Number of samples\n\n    Returns\n    -------\n    samples : jax.array\n        Samples\n    \"\"\"\n    key, subkey = jax.random.split(key)\n\n    selected = jax.random.choice(\n        key,\n        jnp.arange(self.n_components),\n        p=self.weights.flatten(),\n        shape=(n_samples,),\n    )\n\n    # TODO: this blows up the memory, as the arrays are copied\n    means = jnp.take(self.means, selected, axis=Axis.components)\n    covar = jnp.take(self.covariances.values, selected, axis=Axis.components)\n\n    samples = jax.random.multivariate_normal(\n        subkey,\n        jnp.squeeze(means, axis=(Axis.batch, Axis.features_covar)),\n        jnp.squeeze(covar, axis=Axis.batch),\n        shape=(n_samples,),\n    )\n\n    return samples\n</code></pre>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.to_sklearn","title":"<code>to_sklearn(**kwargs)</code>","text":"<p>Convert to sklearn GaussianMixture</p> <p>The methods sets the weights, means, precisions_cholesky and covariances_ attributes, however sklearn will overvwrite them when fitting the model.</p>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.to_sklearn--parameters","title":"Parameters","text":"<p>**kwargs : dict     Additional arguments passed to <code>~sklearn.mixture.GaussianMixture</code></p>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.to_sklearn--returns","title":"Returns","text":"<p>gmm : <code>~sklearn.mixture.GaussianMixture</code>     Gaussian mixture model instance.</p> Source code in <code>gmmx/gmm.py</code> <pre><code>def to_sklearn(self, **kwargs: dict) -&gt; Any:\n    \"\"\"Convert to sklearn GaussianMixture\n\n    The methods sets the weights, means, precisions_cholesky and covariances_ attributes,\n    however sklearn will overvwrite them when fitting the model.\n\n    Parameters\n    ----------\n    **kwargs : dict\n        Additional arguments passed to `~sklearn.mixture.GaussianMixture`\n\n    Returns\n    -------\n    gmm : `~sklearn.mixture.GaussianMixture`\n        Gaussian mixture model instance.\n    \"\"\"\n    from sklearn.mixture import GaussianMixture  # type: ignore [import-untyped]\n\n    gmm = GaussianMixture(\n        n_components=self.n_components,\n        covariance_type=SKLEARN_COVARIANCE_TYPE[type(self.covariances)],\n        **kwargs,\n    )\n    gmm.weights_ = self.weights_numpy\n    gmm.means_ = self.means_numpy\n    gmm.precisions_cholesky_ = self.covariances.precisions_cholesky_numpy\n    gmm.covariances_ = self.covariances.values_numpy\n    return gmm\n</code></pre>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.update_parameters","title":"<code>update_parameters(x, resp, reg_covar)</code>","text":"<p>Update parameters</p>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.update_parameters--parameters","title":"Parameters","text":"<p>x : jax.array     Feature vectors resp : jax.array     Responsibilities reg_covar : float     Regularization for the covariance matrix</p>"},{"location":"modules/#gmmx.gmm.GaussianMixtureModelJax.update_parameters--returns","title":"Returns","text":"<p>gmm : GaussianMixtureModelJax     Updated Gaussian mixture model</p> Source code in <code>gmmx/gmm.py</code> <pre><code>def update_parameters(\n    self, x: jax.Array, resp: jax.Array, reg_covar: float\n) -&gt; GaussianMixtureModelJax:\n    \"\"\"Update parameters\n\n    Parameters\n    ----------\n    x : jax.array\n        Feature vectors\n    resp : jax.array\n        Responsibilities\n    reg_covar : float\n        Regularization for the covariance matrix\n\n    Returns\n    -------\n    gmm : GaussianMixtureModelJax\n        Updated Gaussian mixture model\n    \"\"\"\n    nk = jnp.sum(resp, axis=Axis.batch, keepdims=True)\n    means = jnp.matmul(resp.T, x.T.mT).T / nk\n    covariances = self.covariances.update_parameters(\n        x=x, means=means, resp=resp, nk=nk, reg_covar=reg_covar\n    )\n    return self.__class__(\n        weights=nk / nk.sum(), means=means, covariances=covariances\n    )\n</code></pre>"},{"location":"modules/#gmmx.gmm.check_shape","title":"<code>check_shape(array, expected)</code>","text":"<p>Check shape of array</p> Source code in <code>gmmx/gmm.py</code> <pre><code>def check_shape(array: jax.Array, expected: tuple[int | None, ...]) -&gt; None:\n    \"\"\"Check shape of array\"\"\"\n    if len(array.shape) != len(expected):\n        message = f\"Expected shape {expected}, got {array.shape}\"\n        raise ValueError(message)\n\n    for n, m in zip(array.shape, expected):\n        if m is not None and n != m:\n            message = f\"Expected shape {expected}, got {array.shape}\"\n            raise ValueError(message)\n</code></pre>"}]}